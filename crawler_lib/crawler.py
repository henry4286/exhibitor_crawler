"""
çˆ¬è™«æ ¸å¿ƒæ¨¡å—

ä¸»çˆ¬è™«ç±»ï¼Œåè°ƒå„æ¨¡å—å®Œæˆæ•°æ®æŠ“å–ä»»åŠ¡
"""

import threading
import time
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Optional, List, Dict, Any

from .config_manager import ConfigManager, CrawlerConfig
from .data_parser import DataParser
from .excel_exporter import ExcelExporter
from .http_client import HttpClient
from .utils import get_nested_value

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from unified_logger import (
    console, log_error, log_info, log_request, 
    log_page_progress, log_list_progress, log_contacts_saved
)


class BaseCrawler:
    """
    çˆ¬è™«åŸºç±»
    
    åŒ…å« CompanyCrawler å’Œ DoubleFetchCrawler çš„å…±åŒé€»è¾‘ï¼š
    - é…ç½®åŠ è½½å’Œåˆå§‹åŒ–
    - åˆ—è¡¨é¡µçˆ¬å–ï¼ˆcrawl_pageï¼‰
    - æ•°æ®å»é‡å’ŒéªŒè¯
    - æ–‡ä»¶æ“ä½œ
    - ç»Ÿè®¡ä¿¡æ¯ç®¡ç†
    
    å­ç±»éœ€è¦å®ç°çš„æ–¹æ³•ï¼š
    - crawl(): å…·ä½“çš„çˆ¬å–æµç¨‹
    
    Attributes:
        exhibition_code: å±•ä¼šä»£ç 
        config: çˆ¬è™«é…ç½®
        max_workers: æœ€å¤§çº¿ç¨‹æ•°
        start_page: èµ·å§‹é¡µç 
        exporter: Excelå¯¼å‡ºå™¨
        http_client: HTTPå®¢æˆ·ç«¯
        data_parser: æ•°æ®è§£æå™¨
    """
    
    def __init__(self, exhibition_code: str, max_workers: int = 4, start_page: int = 1):
        """
        åˆå§‹åŒ–çˆ¬è™«åŸºç±»
        
        Args:
            exhibition_code: å±•ä¼šä»£ç 
            max_workers: æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º4
            start_page: èµ·å§‹é¡µç ï¼Œé»˜è®¤ä¸º1
        
        Raises:
            ValueError: å½“å±•ä¼šé…ç½®ä¸å­˜åœ¨æ—¶æŠ›å‡º
        """
        self.exhibition_code = exhibition_code
        self.max_workers = max_workers
        self.start_page = start_page
        
        # åŠ è½½é…ç½®
        config_manager = ConfigManager()
        self.config = config_manager.get_config(exhibition_code)
        
        if self.config is None:
            raise ValueError(f"æœªæ‰¾åˆ°å±•ä¼š '{exhibition_code}' çš„é…ç½®")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.exporter = ExcelExporter()
        self.http_client = HttpClient()
        self.data_parser = DataParser()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self._total_companies = 0
        self._total_pages = 0
        self._stats_lock = threading.Lock()
    
    def _extract_and_parse(
        self,
        response_data: Dict[str, Any],
        items_key: str,
        field_mapping: Optional[Dict[str, str]] = None
    ) -> List[Dict[str, Any]]:
        """
        é€šç”¨æ•°æ®æå–å’Œè§£ææ–¹æ³•
        
        ä»å“åº”ä¸­æå–æ•°æ®åˆ—è¡¨ï¼Œå¹¶å¯é€‰åœ°æ ¹æ®å­—æ®µæ˜ å°„è§£æã€‚
        
        Args:
            response_data: APIå“åº”æ•°æ®
            items_key: æ•°æ®æå–è·¯å¾„ï¼ˆå¦‚ "data.list"ï¼‰
            field_mapping: å­—æ®µæ˜ å°„å­—å…¸ï¼ˆå¦‚ {"Company": "name", "Phone": "phone"}ï¼‰
        
        Returns:
            æ•°æ®åˆ—è¡¨ï¼ˆåŸå§‹æˆ–å·²è§£æï¼‰
        """
        # 1. æå–æ•°æ®åˆ—è¡¨
        items = self.data_parser.extract_items(response_data, items_key)
        
        # 2. å¦‚æœæœ‰å­—æ®µæ˜ å°„ï¼Œè¿›è¡Œè§£æ
        if field_mapping:
            return self.data_parser.parse_items(items, field_mapping)
        
        # 3. å¦åˆ™è¿”å›åŸå§‹items
        return items

    def _make_request(
        self,
        url: str,
        params_str: str = "",
        data_str: str = "",
        headers: Optional[Dict] = None,
        method: str = "GET",
        context: str = "",
        placeholders: Optional[Dict[str, str]] = None
    ) -> dict | list:
        """
        é€šç”¨è¯·æ±‚æ–¹æ³•(é€‚ç”¨äºåˆ—è¡¨é¡µè¯·æ±‚)ï¼šå¤„ç†è¯·æ±‚å‚æ•°ã€å‘é€è¯·æ±‚ã€è®°å½•æ—¥å¿—
        
        è¿™æ˜¯æ‰€æœ‰HTTPè¯·æ±‚çš„ç»Ÿä¸€å…¥å£ï¼Œå°è£…äº†é‡å¤çš„å¤„ç†é€»è¾‘ã€‚
        
        Args:
            url: è¯·æ±‚URLï¼ˆå¯åŒ…å«å ä½ç¬¦ï¼‰
            params_str: URLå‚æ•°å­—ç¬¦ä¸²ï¼ˆå¯åŒ…å«å ä½ç¬¦ï¼‰
            data_str: è¯·æ±‚ä½“å­—ç¬¦ä¸²ï¼ˆå¯åŒ…å«å ä½ç¬¦ï¼‰
            headers: è¯·æ±‚å¤´
            method: è¯·æ±‚æ–¹æ³•ï¼ˆGET/POSTï¼‰
            context: ä¸Šä¸‹æ–‡æè¿°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            placeholders: å ä½ç¬¦å­—å…¸ï¼Œå¦‚ {"{page}": "1", "#company_id": "123"}
        
        Returns:
            å“åº”æ•°æ®ï¼ˆå·²è§£æä¸ºå­—å…¸ï¼‰
        """
        import json
        
        # 1. å¤„ç†å ä½ç¬¦æ›¿æ¢
        if placeholders:
            for placeholder, value in placeholders.items():
                url = url.replace(placeholder, str(value))
                params_str = params_str.replace(placeholder, str(value))
                data_str = data_str.replace(placeholder, str(value))
        
        # 2. è§£æJSONå‚æ•°
        request_params = None
        if params_str and params_str not in ("nan", "{}", "", "None"):
            try:
                request_params = json.loads(params_str)
            except:
                pass
        
        # 3. å‡†å¤‡è¯·æ±‚æ•°æ®
        request_data = self.http_client.prepare_request_data(data_str, headers or {})
        
        # 4. å‘é€è¯·æ±‚ï¼ˆå¸¦é‡è¯•ï¼‰
        response_data = self.http_client.send_request_with_retry(
            url=url,
            method=method,
            headers=headers or {},
            params=request_params,
            data=request_data,
            context=context
        )
        
        return response_data
    
    def crawl_page(self, page: int) -> list[dict]:
        """
        çˆ¬å–å•é¡µæ•°æ®
        
        Args:
            page: é¡µç 
        
        Returns:
           å…¬å¸ä¿¡æ¯åˆ—è¡¨
        """
        if self.config is None:
            return []
        
        # 1. æ„å»ºè¯·æ±‚å‚æ•°
        params_str, data_str = self.http_client.build_request_params(self.config, page)
        skip_count = (page - 1) * 20
        
        # 2. ä½¿ç”¨é€šç”¨è¯·æ±‚æ–¹æ³•
        response_data = self._make_request(
            url=str(self.config.url),
            params_str=params_str,
            data_str=data_str,
            headers=self.config.headers,
            method=self.config.request_method,
            context=f"åˆ—è¡¨é¡µ{page}",
            placeholders={
                "#page": str(page),
                "#skipCount": str(skip_count)
            }
        )
        
        # 3. ä½¿ç”¨é€šç”¨æå–å’Œè§£ææ–¹æ³•
        try:
            company_list = self._extract_and_parse(
                response_data=response_data,
                items_key=self.config.items_key,
                field_mapping=self.config.company_info_keys,
            )
            
            return company_list
        except Exception as e:
            
            log_request(url=str(self.config.url),
            params=params_str,
            data=data_str,
            response=response_data)
            
            raise RuntimeError(f"è§£æç¬¬{page}é¡µæ•°æ®å¤±è´¥: {e}") from e
    
    def _is_same_data(self, data1: list[dict], data2: list[dict]) -> bool:
        """
        æ£€æŸ¥ä¸¤é¡µæ•°æ®æ˜¯å¦ç›¸åŒï¼ˆç”¨äºæ£€æµ‹æ— ç¿»é¡µAPIï¼‰
        
        æ¯”è¾ƒç­–ç•¥ï¼š
        1. é•¿åº¦ç›¸åŒ
        2. ç¬¬ä¸€æ¡å’Œæœ€åä¸€æ¡è®°å½•çš„å…³é”®å­—æ®µç›¸åŒ
        
        Args:
            data1: ç¬¬ä¸€é¡µæ•°æ®
            data2: ç¬¬äºŒé¡µæ•°æ®
        
        Returns:
            Trueè¡¨ç¤ºç›¸åŒï¼ŒFalseè¡¨ç¤ºä¸åŒ
        """
        if len(data1) != len(data2):
            return False
        
        if len(data1) == 0:
            return True
        
        # æ¯”è¾ƒç¬¬ä¸€æ¡è®°å½•
        if not (data1[0] == data2[0]):
            return False
        
        # å¦‚æœæœ‰å¤šæ¡è®°å½•ï¼Œä¹Ÿæ¯”è¾ƒæœ€åä¸€æ¡
        if len(data1) > 1:
            if not (data1[-1] == data2[-1]):
                return False
        
        return True

    def _delete_old_file_if_needed(self):
        """
        å¦‚æœä»ç¬¬ä¸€é¡µå¼€å§‹çˆ¬å–ï¼Œåˆ é™¤æ—§çš„æ•°æ®æ–‡ä»¶
        """
        if self.start_page == 1:
            old_file_path = self.exporter.get_file_path(self.exhibition_code)
            if os.path.exists(old_file_path):
                try:
                    os.remove(old_file_path)
                    log_info(f"å·²åˆ é™¤æ—§æ–‡ä»¶: {old_file_path}")
                except Exception as e:
                    log_error(f"åˆ é™¤æ—§æ–‡ä»¶å¤±è´¥", e)
    
    def _reset_stats(self):
        """
        é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        """
        self._total_companies = 0
        self._total_pages = 0
    
    def _print_summary(self):
        """
        æ‰“å°çˆ¬å–æ±‡æ€»ä¿¡æ¯
        """
        console("\n" + "="*60)
        console("ğŸ“Š çˆ¬å–æ±‡æ€»")
        console("="*60)
        console(f"å±•ä¼šä»£ç : {self.exhibition_code}")
        console(f"æ€»é¡µæ•°: {self._total_pages}")
        console(f"æ€»æ•°æ®æ¡æ•°: {self._total_companies}")
        console("="*60 + "\n")

    def _count_consecutive_empty(self, sorted_pages: list, batch_results: Dict[int, list]) -> int:
        """è®¡ç®—ä»æ‰¹æ¬¡æœ«å°¾å¼€å§‹è¿ç»­ç©ºé¡µçš„æ•°é‡"""
        cnt = 0
        for p in reversed(sorted_pages):
            # ä»…æŠŠæ˜ç¡®çš„ç©ºåˆ—è¡¨ç®—ä½œç©ºé¡µï¼›å¤±è´¥é¡µ(None)ä¸è®¡ä¸ºâ€œç©ºâ€ï¼Œä½†ä¼šä¸­æ–­è¿ç»­è®¡æ•°
            val = batch_results.get(p)
            if val is None:
                break
            if val == []:
                cnt += 1
            else:
                break
        return cnt

    def _is_entire_batch_empty(self, batch_results: Dict[int, list]) -> bool:
        """åˆ¤æ–­æ•´æ‰¹æ•°æ®æ˜¯å¦å…¨éƒ¨ä¸ºç©º"""
        # åªæœ‰å½“è‡³å°‘å­˜åœ¨ä¸€ä¸ªæˆåŠŸè¿”å›çš„åˆ—è¡¨ï¼Œå¹¶ä¸”æ‰€æœ‰æˆåŠŸè¿”å›çš„åˆ—è¡¨å‡ä¸ºç©ºæ—¶ï¼Œæ‰è®¤ä¸ºæ•´æ‰¹ä¸ºç©ºã€‚
        has_list_result = False
        for v in batch_results.values():
            if v is None:
                # å¿½ç•¥å¤±è´¥é¡µ
                continue
            has_list_result = True
            if v:  # éç©ºåˆ—è¡¨
                return False
        return has_list_result

    def _detect_no_pagination(self, sorted_pages: list, batch_results: Dict[int, list]) -> bool:
        """æ£€æµ‹æ‰¹æ¬¡å†…æ˜¯å¦å­˜åœ¨æ— ç¿»é¡µï¼ˆç›¸é‚»é¡µæ•°æ®ç›¸åŒï¼‰çš„è¿¹è±¡"""
        if len(sorted_pages) < 2:
            return False
        first, second = sorted_pages[0], sorted_pages[1]
        # ä»…åœ¨ä¸¤ä¸ªé¡µéƒ½ä¸ºéç©ºçš„åˆ—è¡¨æ—¶æ¯”è¾ƒæ•°æ®ç›¸åŒ
        v1 = batch_results.get(first)
        v2 = batch_results.get(second)
        if isinstance(v1, list) and isinstance(v2, list) and v1 and v2:
            return self._is_same_data(v1, v2)
        return False

    def paginate_batches(
        self,
        start_page: int,
        process_batch_callback,
        batch_size: int=4,
        max_consecutive_empty: int=3
    ) -> bool:
        """
        æ‰¹é‡å¹¶è¡Œåˆ†é¡µå¼•æ“ï¼šè´Ÿè´£å¹¶è¡ŒæŠ“å–ä¸€æ‰¹é¡µé¢ã€ç»Ÿä¸€åœæ­¢åˆ¤å®šï¼Œå¹¶æŠŠæ‰¹æ¬¡ç»“æœäº¤ç»™å›è°ƒå¤„ç†ã€‚

        å›è°ƒç­¾å: process_batch_callback(batch_results: Dict[int, list]) -> bool|None
            - å¦‚æœè¿”å› False åˆ™åœæ­¢åˆ†é¡µï¼›è¿”å› True/None åˆ™ç»§ç»­ã€‚
        """
        has_data = False
        current_batch_start = start_page

        # ä½¿ç”¨å•ä¸ªçº¿ç¨‹æ± å¤ç”¨çº¿ç¨‹èµ„æºï¼Œä¿è¯æ€»å¹¶å‘ä¸º self.max_workers
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            while True:
                batch_end = current_batch_start + batch_size - 1
                batch_results: Dict[int, list] = {}

                future_to_page = {
                    executor.submit(self.crawl_page, page): page
                    for page in range(current_batch_start, batch_end + 1)
                }

                for future in as_completed(future_to_page):
                    page = future_to_page[future]
                    try:
                        items = future.result()
                        batch_results[page] = items
                        if items:
                            has_data = True
                    except Exception as e:
                        log_error(f"å¤„ç†ç¬¬{page}é¡µæ—¶å‘ç”Ÿé”™è¯¯", e)
                        # åŒºåˆ†å¤±è´¥é¡µä¸ç©ºé¡µï¼Œå¤±è´¥é¡µä½¿ç”¨ None æ ‡è®°
                        batch_results[page] = None

                sorted_pages = sorted(batch_results.keys())

                # äº¤ç»™å›è°ƒå¤„ç†æ‰¹æ¬¡ç»“æœ
                try:
                    cont = process_batch_callback(batch_results)
                    if cont is False:
                        break
                except Exception as e:
                    log_error("å¤„ç†æ‰¹æ¬¡å›è°ƒæ—¶å‡ºé”™", e)
                    break

                # åœæ­¢æ¡ä»¶
                if self._count_consecutive_empty(sorted_pages, batch_results) >= max_consecutive_empty:
                    log_info(f"æ£€æµ‹åˆ°è¿ç»­{max_consecutive_empty}é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                    break

                if self._is_entire_batch_empty(batch_results):
                    log_info("æ•´æ‰¹æ•°æ®éƒ½ä¸ºç©ºï¼Œåœæ­¢çˆ¬å–")
                    break

                if self._detect_no_pagination(sorted_pages, batch_results):
                    log_info("æ£€æµ‹åˆ°ç–‘ä¼¼æ— ç¿»é¡µAPIï¼Œåœæ­¢çˆ¬å–")
                    break

                current_batch_start = batch_end + 1

        return has_data

    def paginate_sequential(self, start_page: int, max_consecutive_empty: int, process_page_callback) -> bool:
        """
        é¡ºåºåˆ†é¡µå¼•æ“ï¼šé€é¡µè¯·æ±‚å¹¶äº¤ç»™å›è°ƒå¤„ç†ï¼›å†…éƒ¨ä¼šæ£€æµ‹è¿ç»­ç©ºé¡µä¸ç›¸é‚»é¡µç›¸åŒçš„æƒ…å†µå¹¶åœæ­¢ã€‚

        å›è°ƒç­¾å: process_page_callback(page:int, items:list) -> bool|None
            - å¦‚æœè¿”å› False åˆ™åœæ­¢åˆ†é¡µï¼›è¿”å› True/None åˆ™ç»§ç»­ã€‚
        """
        page = start_page
        has_data = False
        consecutive_empty = 0
        previous_items = None

        while True:
            items = self.crawl_page(page)

            if not items:
                consecutive_empty += 1
                if consecutive_empty >= max_consecutive_empty:
                    log_info(f"è¿ç»­{max_consecutive_empty}é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–")
                    break
                page += 1
                continue

            # æ£€æµ‹ç›¸é‚»é¡µæ˜¯å¦ç›¸åŒï¼ˆç–‘ä¼¼æ— ç¿»é¡µAPIï¼‰
            if previous_items is not None and self._is_same_data(previous_items, items):
                log_info(f"ç¬¬{page}é¡µæ•°æ®ä¸ç¬¬{page-1}é¡µç›¸åŒï¼Œç–‘ä¼¼æ— ç¿»é¡µAPIï¼Œåœæ­¢çˆ¬å–")
                break

            consecutive_empty = 0
            previous_items = items
            has_data = True

            try:
                cont = process_page_callback(page, items)
                if cont is False:
                    break
            except Exception as e:
                log_error(f"å¤„ç†ç¬¬{page}é¡µå›è°ƒæ—¶å‡ºé”™", e)
                break

            page += 1

        return has_data
    
    
    def crawl(self) -> bool:
        """
        æ‰§è¡Œçˆ¬å–æµç¨‹ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œç”±å­ç±»å®ç°ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸè·å–åˆ°æ•°æ®
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° crawl() æ–¹æ³•")


class CompanyCrawler(BaseCrawler):
    """
    å•æ¬¡è¯·æ±‚çˆ¬è™«
    
    é€‚ç”¨äºAPIä¸€æ¬¡æ€§è¿”å›å®Œæ•´æ•°æ®çš„åœºæ™¯ã€‚
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    - åŠ¨æ€ç¿»é¡µæœºåˆ¶ï¼Œä¸é¢„å…ˆæ£€æµ‹æ€»é¡µæ•°
    - å¹¶è¡Œæ¨¡å¼ï¼šæ‰¹é‡çˆ¬å–+åŠ¨æ€æ‰©å±•ï¼Œè‡ªåŠ¨æ¢æµ‹æ•°æ®è¾¹ç•Œ
    """
    
    def crawl_parallel(self) -> bool:
        """
        å¹¶è¡Œçˆ¬å–æ¨¡å¼
        
        ä½¿ç”¨åŠ¨æ€æ‰©å±•ç­–ç•¥ï¼š
        1. æ‰¹é‡çˆ¬å–ä¸€å®šæ•°é‡çš„é¡µé¢
        2. æ ¹æ®ç»“æœåˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­
        3. å¦‚æœæœ€åå‡ é¡µéƒ½æœ‰æ•°æ®ï¼Œç»§ç»­ä¸‹ä¸€æ‰¹
        4. å¦‚æœé‡åˆ°è¿ç»­ç©ºé¡µï¼Œæå‰åœæ­¢
        
        Returns:
            æ˜¯å¦æˆåŠŸè·å–åˆ°æ•°æ®
        """
        if self.config is None:
            return False

        headers = list(self.config.company_info_keys.keys())

        def _process_batch(batch_results: Dict[int, list]):
            # æŒ‰é¡µé¡ºåºä¿å­˜æ•°æ®å¹¶æ›´æ–°ç»Ÿè®¡
            sorted_pages = sorted(batch_results.keys())
            failed_pages = []
            for page in sorted_pages:
                val = batch_results.get(page)
                if val is None:
                    # è¯¥é¡µåœ¨æŠ“å–æˆ–è§£æé˜¶æ®µå¤±è´¥ï¼Œè®°å½•ä»¥ä¾¿åç»­é‡è¯•æˆ–è½¬å‚¨
                    log_error(f"ç¬¬{page}é¡µæŠ“å–/è§£æå¤±è´¥ï¼Œå·²è·³è¿‡ï¼ˆå»ºè®®é‡è¯•æˆ–æŸ¥çœ‹ dumpï¼‰")
                    failed_pages.append(page)
                    continue

                company_list = val or []
                if company_list:
                    try:
                        self.exporter.save(company_list, self.exhibition_code, headers)
                        with self._stats_lock:
                            self._total_companies += len(company_list)
                            self._total_pages += 1
                        log_page_progress(page, len(company_list))
                    except Exception as e:
                        log_error(f"ä¿å­˜ç¬¬{page}é¡µæ•°æ®å‡ºé”™", e)

            # ç»§ç»­åˆ†é¡µï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
            return True

        return self.paginate_batches(
            start_page=self.start_page,
            process_batch_callback=_process_batch
        )
    
    def crawl(self) -> bool:
        """
        æ‰§è¡Œå®Œæ•´çˆ¬å–æµç¨‹ï¼ˆå•æ¬¡è¯·æ±‚æ¨¡å¼ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸè·å–åˆ°æ•°æ®
        """
        try:
           
            # åˆ é™¤æ—§æ–‡ä»¶ï¼ˆå¦‚æœä»ç¬¬ä¸€é¡µå¼€å§‹ï¼‰
            self._delete_old_file_if_needed()
            
            # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
            self._reset_stats()
            
            # æ‰§è¡Œçˆ¬å–
            has_data = self.crawl_parallel()
            
            # æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
            if has_data:
                self._print_summary()
            
            return has_data
            
        except Exception as e:
            log_error("çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯", e)
            return False


class DoubleFetchCrawler(BaseCrawler):
    """
    äºŒæ¬¡è¯·æ±‚çˆ¬è™«ï¼ˆé€é¡µå¤„ç†ç‰ˆï¼‰
    
    é€‚ç”¨äºéœ€è¦å…ˆè·å–åˆ—è¡¨ï¼Œå†è·å–è¯¦æƒ…çš„åœºæ™¯ã€‚
    
    å·¥ä½œæµç¨‹ï¼š
    1. è·å–ä¸€é¡µå…¬å¸åˆ—è¡¨
    2. ç«‹å³æŠ“å–è¿™é¡µæ‰€æœ‰å…¬å¸çš„è”ç³»äºº
    3. ä¿å­˜è¿™ä¸€é¡µçš„è”ç³»äººæ•°æ®åˆ°Excel
    4. ç»§ç»­ä¸‹ä¸€é¡µ
    
    """
    
    def __init__(self, exhibition_code: str, max_workers: int = 4, start_page: int = 1):
        """
        åˆå§‹åŒ–äºŒæ¬¡è¯·æ±‚çˆ¬è™«
        
        Args:
            exhibition_code: å±•ä¼šä»£ç 
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
            start_page: èµ·å§‹é¡µç 
        """
        super().__init__(exhibition_code, max_workers, start_page)
        
        # ä½¿ç”¨DetailFetcheræ¥è·å–è”ç³»äºº
        # æ³¨æ„ï¼šæ­¤æ—¶ self.config å·²ç»åœ¨çˆ¶ç±»åˆå§‹åŒ–æ—¶éªŒè¯è¿‡ï¼Œä¸ä¼šä¸º None
        from .detail_fetcher import DetailFetcher
        self.detail_fetcher = DetailFetcher(self.config, max_workers=self.max_workers)
        
        # äºŒæ¬¡è¯·æ±‚æ¨¡å¼çš„é¢å¤–ç»Ÿè®¡
        self._total_contacts = 0
    
    def _print_double_summary(self):
        """
        æ‰“å°äºŒæ¬¡è¯·æ±‚çˆ¬å–æ±‡æ€»ä¿¡æ¯
        """
        console("\n" + "="*60)
        console("ğŸ“Š çˆ¬å–æ±‡æ€»")
        console("="*60)
        console(f"å±•ä¼šä»£ç : {self.exhibition_code}")
        console(f"æ€»é¡µæ•°: {self._total_pages}")
        console(f"æ€»å…¬å¸æ•°: {self._total_companies}")
        console(f"æ€»è”ç³»äººæ•°: {self._total_contacts}")
        console("="*60 + "\n")
    
    def crawl(self) -> bool:
        """
        æ‰§è¡Œçˆ¬å–æµç¨‹ï¼ˆäºŒæ¬¡è¯·æ±‚æ¨¡å¼ - é€é¡µå¤„ç†ï¼‰
        
        æ¯è·å–ä¸€é¡µå…¬å¸åˆ—è¡¨ï¼Œå°±ç«‹å³æŠ“å–è”ç³»äººå¹¶ä¿å­˜ï¼Œé¿å…æ•°æ®ä¸¢å¤±ã€‚
        
        Returns:
            æ˜¯å¦æˆåŠŸè·å–åˆ°æ•°æ®
        """

        # ç¡®å®šè¡¨å¤´ - åŸºæœ¬é…ç½®çš„å­—æ®µæ˜ å°„ + è”ç³»äººå­—æ®µæ˜ å°„
        if self.config.info_key:
            headers = list(self.config.company_info_keys.keys()) + list(self.config.info_key.keys())
        else:
            headers = list(self.config.company_info_keys.keys())

        # å›è°ƒï¼šé€é¡µå¤„ç†ï¼ˆæŠ“å–è”ç³»äººå¹¶ä¿å­˜ï¼‰
        def _process_page(page: int, items: list):
            log_list_progress(page, len(items))

            # æŠ“å–è”ç³»äºº
            all_contacts = self.detail_fetcher.fetch_batch_contacts_with_basic_info(
                companies_basic_info=items
            )

            if all_contacts:
                try:
                    self.exporter.save(all_contacts, self.exhibition_code, headers)
                    self._total_contacts += len(all_contacts)
                    log_contacts_saved(page, len(all_contacts))
                except Exception as e:
                    log_error(f"ä¿å­˜ç¬¬{page}é¡µè”ç³»äººæ•°æ®å¤±è´¥", e)

            # æ›´æ–°å…¬å¸æ•°ç»Ÿè®¡ï¼ˆä¿æŒåŸè¡Œä¸ºï¼‰
            self._total_companies += len(items)
            self._total_pages += 1

            # ç»§ç»­åˆ†é¡µé»˜è®¤
            return True

        try:
            # åˆ é™¤æ—§æ–‡ä»¶ï¼ˆå¦‚æœä»ç¬¬ä¸€é¡µå¼€å§‹ï¼‰
            self._delete_old_file_if_needed()
            
            # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
            self._reset_stats()

            has_data = self.paginate_sequential(
                start_page=self.start_page,
                max_consecutive_empty=3,
                process_page_callback=_process_page
            )
            
            # æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
            if has_data:
                self._print_double_summary()

            return has_data

        except KeyboardInterrupt:
            log_error("ç”¨æˆ·ä¸­æ–­ï¼Œå·²ä¿å­˜çš„æ•°æ®ä¸ä¼šä¸¢å¤±")
        except Exception as e:
            log_error("çˆ¬å–è¿‡ç¨‹å‡ºé”™", e)

        return False
