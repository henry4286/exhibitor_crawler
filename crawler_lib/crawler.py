"""
çˆ¬è™«æ ¸å¿ƒæ¨¡å—

ä¸»çˆ¬è™«ç±»ï¼Œåè°ƒå„æ¨¡å—å®Œæˆæ•°æ®æŠ“å–ä»»åŠ¡
"""

import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Optional

from .config_manager import ConfigManager, CrawlerConfig
from .data_parser import DataParser
from .excel_exporter import ExcelExporter
from .http_client import HttpClient
from .utils import write_status_file,get_nested_value
from typing import List, Dict, Any

class CompanyCrawler:
    """
    å…¬å¸ä¿¡æ¯çˆ¬è™«
    
    ä¸»çˆ¬è™«ç±»ï¼Œåè°ƒé…ç½®åŠ è½½ã€è¯·æ±‚å‘é€ã€æ•°æ®è§£æå’Œç»“æœä¿å­˜ã€‚
    æ”¯æŒå¤šçº¿ç¨‹å¹¶è¡ŒæŠ“å–ä»¥æé«˜æ•ˆç‡ã€‚
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    - åŠ¨æ€ç¿»é¡µæœºåˆ¶ï¼Œä¸é¢„å…ˆæ£€æµ‹æ€»é¡µæ•°
    - é¡ºåºæ¨¡å¼ï¼šæŒç»­çˆ¬å–ç›´åˆ°é‡åˆ°ç©ºæ•°æ®
    - å¹¶è¡Œæ¨¡å¼ï¼šæ‰¹é‡çˆ¬å–+åŠ¨æ€æ‰©å±•ï¼Œè‡ªåŠ¨æ¢æµ‹æ•°æ®è¾¹ç•Œ
    
    Attributes:
        exhibition_code: å±•ä¼šä»£ç 
        config: çˆ¬è™«é…ç½®
        exporter: Excelå¯¼å‡ºå™¨
        max_workers: æœ€å¤§çº¿ç¨‹æ•°
    """
    
    def __init__(self, exhibition_code: str, max_workers: int = 4, start_page: int = 1):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            exhibition_code: å±•ä¼šä»£ç 
            max_workers: æœ€å¤§çº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º4
            start_page: èµ·å§‹é¡µç ï¼Œé»˜è®¤ä¸º1
        
        Raises:
            ValueError: å½“å±•ä¼šé…ç½®ä¸å­˜åœ¨æ—¶æŠ›å‡º
        """
        self.exhibition_code = exhibition_code
        self.max_workers = max_workers
        self.start_page = start_page
        
        config_manager = ConfigManager()
        self.config = config_manager.get_config(exhibition_code)
        
        if self.config is None:
            raise ValueError(f"æœªæ‰¾åˆ°å±•ä¼š '{exhibition_code}' çš„é…ç½®")
        
        self.exporter = ExcelExporter()
        self.http_client = HttpClient()
        self.data_parser = DataParser()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self._total_companies = 0
        self._total_pages = 0
        self._stats_lock = threading.Lock()
    
    def crawl_page(self, page: int) -> tuple[list[dict], bool]:
        """
        çˆ¬å–å•é¡µæ•°æ®ï¼ˆå¸¦æ— é™é‡è¯•ï¼‰
        
        Args:
            page: é¡µç 
        
        Returns:
            (å…¬å¸ä¿¡æ¯åˆ—è¡¨, æ˜¯å¦æˆåŠŸ)
        """
        if self.config is None:
            return [], False
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params_str, data_str = self.http_client.build_request_params(self.config, page)
        
        # å¤„ç†URLå ä½ç¬¦
        url = str(self.config.url)
        skip_count = (page - 1) * 20
        url = url.replace("{page}", str(page))
        url = url.replace("{skipCount}", str(skip_count))
        url = url.replace("{pageSize}", "20")
        
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        import json
        request_params = None
        if params_str not in ("nan", "{}", "", "None"):
            try:
                request_params = json.loads(params_str)
            except:
                pass
        
        request_data = self.http_client.prepare_request_data(data_str, self.config.headers)
        
        # ä½¿ç”¨å¸¦é‡è¯•çš„è¯·æ±‚æ–¹æ³•
        response_data = self.http_client.send_request_with_retry(
            url=url,
            method=self.config.request_method,
            headers=self.config.headers,
            params=request_params,
            data=request_data if isinstance(request_data, dict) else None,
            context=f"åˆ—è¡¨é¡µ{page}"
        )
        
        # æå–å…¬å¸åˆ—è¡¨
        items = self.data_parser.extract_items(response_data, self.config.items_key)
        
        # è§£æå…¬å¸ä¿¡æ¯
        company_list = self.data_parser.parse_company_info(items, self.config.company_info_keys)
        
        return company_list, True
    
    def crawl_sequential(self) -> bool:
        """
        é¡ºåºçˆ¬å–æ¨¡å¼
        
        æŒç»­çˆ¬å–ç›´åˆ°é‡åˆ°ç©ºæ•°æ®é¡µæˆ–è¿ç»­å¤šä¸ªç©ºæ•°æ®é¡µã€‚
        è¿™æ˜¯æœ€å¯é çš„æ–¹å¼ï¼Œé€‚åˆæ•°æ®é‡ä¸ç¡®å®šçš„æƒ…å†µã€‚
        
        Returns:
            æ˜¯å¦æˆåŠŸè·å–åˆ°æ•°æ®
        """
        print(f"ä½¿ç”¨é¡ºåºçˆ¬å–æ¨¡å¼", flush=True)
        if self.start_page > 1:
            print(f"ğŸ“ ä»ç¬¬ {self.start_page} é¡µå¼€å§‹æŠ“å–", flush=True)
        
        if self.config is None:
            return False
        
        page = self.start_page
        has_data = False
        consecutive_empty = 0
        max_consecutive_empty = 3  # è¿ç»­3é¡µç©ºæ•°æ®æ‰åœæ­¢
        headers = list(self.config.company_info_keys.keys())
        previous_data = None  # ç”¨äºæ£€æµ‹é‡å¤æ•°æ®
        
        while True:
            try:
                print(f"æ­£åœ¨ä¸‹è½½ç¬¬{page}é¡µçš„æ•°æ®", flush=True)
                write_status_file("python_excute_status.txt", f"æ­£åœ¨ä¸‹è½½ç¬¬{page}é¡µçš„æ•°æ®")
                
                company_list, success = self.crawl_page(page)
                
                if company_list:
                    # æ£€æŸ¥æ˜¯å¦ä¸å‰ä¸€é¡µæ•°æ®å®Œå…¨ç›¸åŒï¼ˆé¿å…æ— ç¿»é¡µAPIçš„æ­»å¾ªç¯ï¼‰
                    if previous_data is not None and self._is_same_data(previous_data, company_list):
                        print(f"âš ï¸  ç¬¬{page}é¡µæ•°æ®ä¸ç¬¬{page-1}é¡µç›¸åŒï¼Œç–‘ä¼¼æ— ç¿»é¡µAPIï¼Œåœæ­¢çˆ¬å–", flush=True)
                        break
                    
                    # æœ‰æ•°æ®ï¼Œä¿å­˜å¹¶ç»§ç»­
                    self.exporter.save(company_list, self.exhibition_code, headers)
                    has_data = True
                    consecutive_empty = 0
                    self._total_companies += len(company_list)
                    self._total_pages += 1
                    print(f"ç¬¬{page}é¡µå®Œæˆï¼Œè·å–åˆ°{len(company_list)}æ¡æ•°æ®", flush=True)
                    
                    # ä¿å­˜å½“å‰é¡µæ•°æ®ç”¨äºä¸‹æ¬¡æ¯”è¾ƒ
                    previous_data = company_list
                    page += 1
                else:
                    # ç©ºæ•°æ®
                    consecutive_empty += 1
                    print(f"ç¬¬{page}é¡µæ— æ•°æ®ï¼ˆè¿ç»­ç©ºé¡µ: {consecutive_empty}/{max_consecutive_empty}ï¼‰", flush=True)
                    
                    if consecutive_empty >= max_consecutive_empty:
                        print(f"è¿ç»­{max_consecutive_empty}é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–", flush=True)
                        break
                    
                    page += 1
                    
            except Exception as e:
                error_msg = f"{self.exhibition_code}ç¬¬{page}é¡µæ•°æ®ä¸‹è½½å¤±è´¥: {e}"
                print(error_msg, flush=True)
                write_status_file("errorlog.txt", error_msg)
                
                consecutive_empty += 1
                if consecutive_empty >= max_consecutive_empty:
                    break
                
                page += 1
        
        return has_data
    
    def _is_same_data(self, data1: list[dict], data2: list[dict]) -> bool:
        """
        æ£€æŸ¥ä¸¤é¡µæ•°æ®æ˜¯å¦ç›¸åŒï¼ˆç”¨äºæ£€æµ‹æ— ç¿»é¡µAPIï¼‰
        
        æ¯”è¾ƒç­–ç•¥ï¼š
        1. é•¿åº¦ç›¸åŒ
        2. ç¬¬ä¸€æ¡å’Œæœ€åä¸€æ¡è®°å½•çš„å…³é”®å­—æ®µç›¸åŒ
        """
        if len(data1) != len(data2):
            return False
        
        if len(data1) == 0:
            return True
        
        # æ¯”è¾ƒç¬¬ä¸€æ¡è®°å½•
        if not self._compare_records(data1[0], data2[0]):
            return False
        
        # å¦‚æœæœ‰å¤šæ¡è®°å½•ï¼Œä¹Ÿæ¯”è¾ƒæœ€åä¸€æ¡
        if len(data1) > 1:
            if not self._compare_records(data1[-1], data2[-1]):
                return False
        
        return True
    
    def _compare_records(self, record1: dict, record2: dict) -> bool:
        """
        æ¯”è¾ƒä¸¤æ¡è®°å½•çš„å…³é”®å­—æ®µæ˜¯å¦ç›¸åŒ
        
        é€‰æ‹©3-5ä¸ªå…³é”®å­—æ®µè¿›è¡Œæ¯”è¾ƒï¼Œé¿å…å…¨é‡æ¯”è¾ƒçš„æ€§èƒ½é—®é¢˜
        """
        # è·å–æ‰€æœ‰å­—æ®µ
        keys = list(record1.keys())[:5]  # å–å‰5ä¸ªå­—æ®µæ¯”è¾ƒ
        
        for key in keys:
            if record1.get(key) != record2.get(key):
                return False
        
        return True
    
    def crawl_parallel(self) -> bool:
        """
        å¹¶è¡Œçˆ¬å–æ¨¡å¼
        
        ä½¿ç”¨åŠ¨æ€æ‰©å±•ç­–ç•¥ï¼š
        1. æ‰¹é‡çˆ¬å–ä¸€å®šæ•°é‡çš„é¡µé¢
        2. æ ¹æ®ç»“æœåˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­
        3. å¦‚æœæœ€åå‡ é¡µéƒ½æœ‰æ•°æ®ï¼Œç»§ç»­ä¸‹ä¸€æ‰¹
        4. å¦‚æœé‡åˆ°è¿ç»­ç©ºé¡µï¼Œæå‰åœæ­¢
        
        Returns:
            æ˜¯å¦æˆåŠŸè·å–åˆ°æ•°æ®
        """
        print(f"ä½¿ç”¨å¹¶è¡Œçˆ¬å–æ¨¡å¼ï¼Œçº¿ç¨‹æ•°: {self.max_workers}", flush=True)
        if self.start_page > 1:
            print(f"ğŸ“ ä»ç¬¬ {self.start_page} é¡µå¼€å§‹æŠ“å–", flush=True)
        
        if self.config is None:
            return False
        
        headers = list(self.config.company_info_keys.keys())
        has_data = False
        
        # æ‰¹é‡çˆ¬å–å‚æ•°
        batch_size = 10  # æ¯æ‰¹çˆ¬å–10é¡µ
        current_batch_start = self.start_page
        max_consecutive_empty = 3  # è¿ç»­ç©ºé¡µé˜ˆå€¼
        
        while True:
            batch_end = current_batch_start + batch_size - 1
            print(f"\nå¼€å§‹çˆ¬å–ç¬¬ {current_batch_start}-{batch_end} é¡µ", flush=True)
            
            # ä½¿ç”¨çº¿ç¨‹æ± çˆ¬å–å½“å‰æ‰¹æ¬¡
            batch_results = {}
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰é¡µé¢ä»»åŠ¡
                future_to_page = {
                    executor.submit(self.crawl_page, page): page 
                    for page in range(current_batch_start, batch_end + 1)
                }
                
                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for future in as_completed(future_to_page):
                    page = future_to_page[future]
                    
                    try:
                        company_list, success = future.result()
                        batch_results[page] = company_list
                        
                        if company_list:
                            self.exporter.save(company_list, self.exhibition_code, headers)
                            has_data = True
                            
                            with self._stats_lock:
                                self._total_companies += len(company_list)
                                self._total_pages += 1
                            
                            print(f"ç¬¬{page}é¡µå®Œæˆï¼Œè·å–åˆ°{len(company_list)}æ¡æ•°æ®", flush=True)
                        else:
                            print(f"ç¬¬{page}é¡µæ— æ•°æ®", flush=True)
                        
                        # æ›´æ–°çŠ¶æ€
                        write_status_file("python_excute_status.txt", 
                                        f"å·²å®Œæˆç¬¬{page}é¡µï¼Œå…±è·å–{self._total_companies}æ¡æ•°æ®")
                        
                    except Exception as e:
                        error_msg = f"å¤„ç†ç¬¬{page}é¡µæ—¶å‘ç”Ÿé”™è¯¯: {e}"
                        print(error_msg, flush=True)
                        write_status_file("errorlog.txt", error_msg)
                        batch_results[page] = []
            
            # åˆ†ææ‰¹æ¬¡ç»“æœï¼Œå†³å®šæ˜¯å¦ç»§ç»­
            # æ£€æŸ¥æœ€åå‡ é¡µæ˜¯å¦éƒ½ä¸ºç©º
            sorted_pages = sorted(batch_results.keys())
            consecutive_empty_count = 0
            
            for page in reversed(sorted_pages):
                if not batch_results[page]:
                    consecutive_empty_count += 1
                else:
                    break
            
            print(f"æ‰¹æ¬¡å®Œæˆï¼Œæœ€åè¿ç»­ç©ºé¡µæ•°: {consecutive_empty_count}", flush=True)
            
            # å¦‚æœè¿ç»­ç©ºé¡µæ•°è¾¾åˆ°é˜ˆå€¼ï¼Œåœæ­¢çˆ¬å–
            if consecutive_empty_count >= max_consecutive_empty:
                print(f"æ£€æµ‹åˆ°è¿ç»­{consecutive_empty_count}é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–", flush=True)
                break
            
            # å¦‚æœæ•´æ‰¹éƒ½æ˜¯ç©ºçš„ï¼Œä¹Ÿåœæ­¢
            if all(not batch_results[p] for p in sorted_pages):
                print("æ•´æ‰¹æ•°æ®éƒ½ä¸ºç©ºï¼Œåœæ­¢çˆ¬å–", flush=True)
                break
            
            # æ£€æµ‹æ‰¹æ¬¡ä¸­æ˜¯å¦æœ‰é‡å¤æ•°æ®ï¼ˆæ— ç¿»é¡µAPIæ£€æµ‹ï¼‰
            if len(sorted_pages) >= 2:
                # æ¯”è¾ƒæ‰¹æ¬¡ä¸­ç¬¬ä¸€é¡µå’Œç¬¬äºŒé¡µçš„æ•°æ®
                first_page = sorted_pages[0]
                second_page = sorted_pages[1]
                if (batch_results[first_page] and batch_results[second_page] and 
                    self._is_same_data(batch_results[first_page], batch_results[second_page])):
                    print(f"âš ï¸  æ£€æµ‹åˆ°ç¬¬{first_page}é¡µå’Œç¬¬{second_page}é¡µæ•°æ®ç›¸åŒï¼Œç–‘ä¼¼æ— ç¿»é¡µAPIï¼Œåœæ­¢çˆ¬å–", flush=True)
                    break
            
            # ç»§ç»­ä¸‹ä¸€æ‰¹
            current_batch_start = batch_end + 1
        
        return has_data
    
    def crawl(self, use_parallel: bool = True) -> bool:
        """
        æ‰§è¡Œå®Œæ•´çˆ¬å–æµç¨‹
        
        Args:
            use_parallel: æ˜¯å¦ä½¿ç”¨å¹¶è¡ŒæŠ“å–ï¼Œé»˜è®¤True
        
        Returns:
            æ˜¯å¦æˆåŠŸè·å–åˆ°æ•°æ®
        """
        try:
            start_time = time.time()
            
            # æ¸…ç©ºç»Ÿè®¡ä¿¡æ¯
            self._total_companies = 0
            self._total_pages = 0
            
            # æ‰§è¡Œçˆ¬å–
            if use_parallel and self.max_workers > 1:
                has_data = self.crawl_parallel()
            else:
                has_data = self.crawl_sequential()
            
            # è®¡ç®—è€—æ—¶
            elapsed_time = time.time() - start_time
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            print(f"\n{'='*50}", flush=True)
            print(f"çˆ¬å–å®Œæˆï¼", flush=True)
            print(f"æˆåŠŸé¡µæ•°: {self._total_pages}", flush=True)
            print(f"æ€»æ•°æ®é‡: {self._total_companies} æ¡", flush=True)
            print(f"è€—æ—¶: {elapsed_time:.2f} ç§’", flush=True)
            print(f"{'='*50}\n", flush=True)
            
            # æ›´æ–°æœ€ç»ˆçŠ¶æ€
            status = "å®Œæˆ" if has_data else "å¤±è´¥"
            write_status_file("python_excute_status.txt", status)
            
            return has_data
            
        except Exception as e:
            error_msg = f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}"
            print(error_msg, flush=True)
            write_status_file("errorlog.txt", error_msg)
            write_status_file("python_excute_status.txt", "å¤±è´¥")
            return False


class DoubleFetchCrawler:
    """
    äºŒæ¬¡è¯·æ±‚çˆ¬è™«ï¼ˆé€é¡µå¤„ç†ç‰ˆï¼‰
    
    å·¥ä½œæµç¨‹ï¼š
    1. è·å–ä¸€é¡µå…¬å¸åˆ—è¡¨
    2. ç«‹å³æŠ“å–è¿™é¡µæ‰€æœ‰å…¬å¸çš„è”ç³»äºº
    3. ä¿å­˜è¿™ä¸€é¡µçš„è”ç³»äººæ•°æ®åˆ°Excel
    4. ç»§ç»­ä¸‹ä¸€é¡µ
    
    ä¼˜ç‚¹ï¼šä¸ä¼šå› ä¸ºåé¢æŸä¸ªè”ç³»äººå¤±è´¥è€Œä¸¢å¤±å‰é¢çš„æ•°æ®
    """
    
    def __init__(self, exhibition_code: str, max_workers: int = 4, start_page: int = 1):
        self.exhibition_code = exhibition_code
        self.max_workers = max_workers
        self.start_page = start_page
        
        config_manager = ConfigManager()
        self.config = config_manager.get_config(exhibition_code)
        
        if self.config is None:
            raise ValueError(f"æœªæ‰¾åˆ°å±•ä¼šé…ç½®")
        
        self.http_client = HttpClient()
        self.data_parser = DataParser()
        self.exporter = ExcelExporter()
        
        # ä½¿ç”¨DetailFetcheræ¥è·å–è”ç³»äººï¼ˆä¸test_config.pyä½¿ç”¨ç›¸åŒçš„æ–¹æ³•ï¼‰
        from .detail_fetcher import DetailFetcher
        self.detail_fetcher = DetailFetcher(self.config, max_workers=self.max_workers)
        
        self._total_companies = 0
        self._total_contacts = 0
    
    def crawl_page(self, page: int) -> List[Dict[str, Any]]:
        """è·å–å…¬å¸åˆ—è¡¨é¡µï¼ˆå¸¦æ— é™é‡è¯•ï¼‰"""
        if self.config is None:
            return []
        
        import json
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params_str, data_str = self.http_client.build_request_params(self.config, page)
        
        # å¤„ç†URLå ä½ç¬¦
        url = str(self.config.url)
        skip_count = (page - 1) * 20
        url = url.replace("{page}", str(page))
        url = url.replace("{skipCount}", str(skip_count))
        url = url.replace("{pageSize}", "20")
        
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        request_params = None
        if params_str not in ("nan", "{}", "", "None"):
            try:
                request_params = json.loads(params_str)
            except:
                pass
        
        request_data = self.http_client.prepare_request_data(data_str, self.config.headers)
        
        # ä½¿ç”¨å¸¦é‡è¯•çš„è¯·æ±‚æ–¹æ³•
        response_data = self.http_client.send_request_with_retry(
            url=url,
            method=self.config.request_method,
            headers=self.config.headers,
            params=request_params,
            data=request_data if isinstance(request_data, dict) else None,
            context=f"åˆ—è¡¨é¡µ{page}"
        )
        
        items = self.data_parser.extract_items(response_data, self.config.items_key)
        return items if isinstance(items, list) else []
    
    def crawl(self) -> bool:
        """
        æ‰§è¡Œçˆ¬å– - é€é¡µå¤„ç†æ¨¡å¼
        
        æ¯è·å–ä¸€é¡µå…¬å¸åˆ—è¡¨ï¼Œå°±ç«‹å³æŠ“å–è”ç³»äººå¹¶ä¿å­˜ï¼Œé¿å…æ•°æ®ä¸¢å¤±
        """
        if self.config is None:
            print("âŒ é…ç½®æœªåŠ è½½", flush=True)
            return False
            
        print(f"ğŸš€ å¼€å§‹äºŒæ¬¡è¯·æ±‚çˆ¬å–ï¼ˆé€é¡µå¤„ç†æ¨¡å¼ï¼‰", flush=True)
        print(f"   - å¹¶å‘çº¿ç¨‹æ•°: {self.max_workers}", flush=True)
        print(f"   - ç­–ç•¥: ä¸€é¡µä¸€é¡µå¤„ç†ï¼Œç«‹å³ä¿å­˜", flush=True)
        if self.start_page > 1:
            print(f"ğŸ“ ä»ç¬¬ {self.start_page} é¡µå¼€å§‹æŠ“å–", flush=True)
        
        page = self.start_page
        has_data = False
        consecutive_empty = 0
        previous_companies = None  # ç”¨äºæ£€æµ‹é‡å¤æ•°æ®
        
        # ç¡®å®šè¡¨å¤´
        if self.config.info_key:
            headers = ["company_name"] + list(self.config.info_key.keys())
        else:
            headers = list(self.config.company_info_keys.keys())
        
        try:
            while True:
                print(f"\n{'='*60}", flush=True)
                print(f"ğŸ“„ ç¬¬{page}é¡µ - æ­¥éª¤1: è·å–å…¬å¸åˆ—è¡¨", flush=True)
                print(f"{'='*60}", flush=True)
                
                # æ­¥éª¤1: è·å–è¿™ä¸€é¡µçš„å…¬å¸åˆ—è¡¨
                companies = self.crawl_page(page)
                
                if not companies:
                    consecutive_empty += 1
                    print(f"âš ï¸  ç¬¬{page}é¡µæ— æ•°æ®ï¼ˆè¿ç»­ç©ºé¡µ: {consecutive_empty}/3ï¼‰", flush=True)
                    if consecutive_empty >= 3:
                        print("âœ‹ è¿ç»­3é¡µæ— æ•°æ®ï¼Œåœæ­¢çˆ¬å–", flush=True)
                        break
                    page += 1
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä¸å‰ä¸€é¡µæ•°æ®å®Œå…¨ç›¸åŒï¼ˆé¿å…æ— ç¿»é¡µAPIçš„æ­»å¾ªç¯ï¼‰
                if previous_companies is not None and self._is_same_companies(previous_companies, companies):
                    print(f"âš ï¸  ç¬¬{page}é¡µæ•°æ®ä¸ç¬¬{page-1}é¡µç›¸åŒï¼Œç–‘ä¼¼æ— ç¿»é¡µAPIï¼Œåœæ­¢çˆ¬å–", flush=True)
                    break
                
                consecutive_empty = 0
                print(f"âœ… è·å–åˆ° {len(companies)} ä¸ªå…¬å¸", flush=True)
                
                # æ­¥éª¤2: ç«‹å³æŠ“å–è¿™ä¸€é¡µå…¬å¸çš„è”ç³»äºº
                print(f"\nğŸ“ ç¬¬{page}é¡µ - æ­¥éª¤2: æŠ“å– {len(companies)} ä¸ªå…¬å¸çš„è”ç³»äººï¼ˆ{self.max_workers}çº¿ç¨‹å¹¶å‘ï¼‰", flush=True)
                
                # ä½¿ç”¨DetailFetcherçš„fetch_batch_detailsæ–¹æ³•ï¼ˆä¸test_config.pyç›¸åŒï¼‰
                all_contacts = self.detail_fetcher.fetch_batch_details(
                    companies, 
                    fetch_contacts=True  # è·å–è”ç³»äººæ¨¡å¼
                )
                
                # æ­¥éª¤3: ç«‹å³ä¿å­˜è¿™ä¸€é¡µçš„è”ç³»äººæ•°æ®
                if all_contacts:
                    print(f"\nğŸ’¾ ç¬¬{page}é¡µ - æ­¥éª¤3: ä¿å­˜ {len(all_contacts)} æ¡è”ç³»äººåˆ°Excel", flush=True)
                    self.exporter.save(all_contacts, self.exhibition_code, headers)
                    self._total_contacts += len(all_contacts)
                    has_data = True
                    print(f"âœ… ç¬¬{page}é¡µæ•°æ®å·²å®‰å…¨ä¿å­˜ï¼", flush=True)
                else:
                    print(f"âš ï¸  ç¬¬{page}é¡µæœªè·å–åˆ°è”ç³»äººæ•°æ®", flush=True)
                
                # æ›´æ–°ç»Ÿè®¡
                self._total_companies += len(companies)
                previous_companies = companies  # ä¿å­˜å½“å‰é¡µæ•°æ®ç”¨äºä¸‹æ¬¡æ¯”è¾ƒ
                
                # è¾“å‡ºå½“å‰è¿›åº¦
                print(f"\nğŸ“Š ç´¯è®¡è¿›åº¦: å·²å¤„ç† {self._total_companies} ä¸ªå…¬å¸ï¼Œè·å– {self._total_contacts} æ¡è”ç³»äºº", flush=True)
                
                # ç»§ç»­ä¸‹ä¸€é¡µï¼ˆæ— å»¶è¿Ÿï¼Œé€Ÿåº¦ä¼˜å…ˆï¼‰
                page += 1
                
        except KeyboardInterrupt:
            print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œå·²ä¿å­˜çš„æ•°æ®ä¸ä¼šä¸¢å¤±", flush=True)
        except Exception as e:
            print(f"\nâŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}", flush=True)
            import traceback
            traceback.print_exc()
            print(f"âš ï¸  å·²ä¿å­˜çš„æ•°æ®ä¸ä¼šä¸¢å¤±", flush=True)
        
        # æœ€ç»ˆç»Ÿè®¡
        print(f"\n{'='*60}", flush=True)
        print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼", flush=True)
        print(f"{'='*60}", flush=True)
        print(f"æ€»é¡µæ•°: {page - self.start_page}", flush=True)
        print(f"æ€»å…¬å¸æ•°: {self._total_companies}", flush=True)
        print(f"æ€»è”ç³»äºº: {self._total_contacts}", flush=True)
        print(f"æ•°æ®æ–‡ä»¶: ExhibitorList/{self.exhibition_code}.xlsx", flush=True)
        print(f"{'='*60}\n", flush=True)
        
        return has_data
    
    def _is_same_companies(self, companies1: List[Dict[str, Any]], companies2: List[Dict[str, Any]]) -> bool:
        """
        æ£€æŸ¥ä¸¤é¡µå…¬å¸æ•°æ®æ˜¯å¦ç›¸åŒï¼ˆç”¨äºæ£€æµ‹æ— ç¿»é¡µAPIï¼‰
        
        æ¯”è¾ƒç­–ç•¥ï¼š
        1. é•¿åº¦ç›¸åŒ
        2. ç¬¬ä¸€æ¡å’Œæœ€åä¸€æ¡è®°å½•çš„å…³é”®å­—æ®µç›¸åŒ
        """
        if self.config is None:
            return False
            
        if len(companies1) != len(companies2):
            return False
        
        if len(companies1) == 0:
            return True
        
        # æ¯”è¾ƒç¬¬ä¸€ä¸ªå…¬å¸çš„IDæˆ–åç§°
        id_key = self.config.id_key or "id"
        name_key = self.config.company_name_key or "name"
        
        first1_id = get_nested_value(companies1[0], id_key)
        first2_id = get_nested_value(companies2[0], id_key)
        
        if first1_id and first2_id and first1_id == first2_id:
            # å¦‚æœæœ‰å¤šä¸ªå…¬å¸ï¼Œä¹Ÿæ¯”è¾ƒæœ€åä¸€ä¸ª
            if len(companies1) > 1:
                last1_id = get_nested_value(companies1[-1], id_key)
                last2_id = get_nested_value(companies2[-1], id_key)
                if last1_id and last2_id and last1_id == last2_id:
                    return True
            else:
                return True
        
        return False
