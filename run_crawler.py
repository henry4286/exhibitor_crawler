"""
ç»Ÿä¸€çˆ¬è™«ç¨‹åº - æ”¯æŒå•æ¬¡è¯·æ±‚å’ŒäºŒæ¬¡è¯·æ±‚ä¸¤ç§æ¨¡å¼

ä½¿ç”¨æ–¹æ³•:
    python run_crawler.py <exhibition_code> [options]
    
ç¤ºä¾‹:
    # å•æ¬¡è¯·æ±‚æ¨¡å¼ï¼ˆè‡ªåŠ¨æ ¹æ®é…ç½®åˆ¤æ–­ï¼‰
    python run_crawler.py æ— äººæœºå±•
    
    # æŒ‡å®šçº¿ç¨‹æ•°
    python run_crawler.py æ— äººæœºå±• --workers 8
    
    # äºŒæ¬¡è¯·æ±‚æ¨¡å¼ä¼šè‡ªåŠ¨è¯†åˆ«ï¼ˆæ ¹æ®config.xlsxä¸­çš„request_modeå­—æ®µï¼‰
    python run_crawler.py å†œäº§å“
"""

import sys
import time
import urllib3
from typing import List, Dict, Any
from datetime import datetime

# è®¾ç½®UTF-8ç¼–ç 
import locale
import io

# é‡è®¾æ ‡å‡†è¾“å‡ºä¸ºUTF-8ç¼–ç ï¼Œå¹¶ç¡®ä¿æ— ç¼“å†²
if sys.platform == 'win32':
    # Windowsç³»ç»Ÿç‰¹æ®Šå¤„ç†
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', line_buffering=True, errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', line_buffering=True, errors='replace')
else:
    # Linux/Macç³»ç»Ÿ
    sys.stdout.reconfigure(encoding='utf-8', line_buffering=True, errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', line_buffering=True, errors='replace')

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from unified_logger import log_info, log_error, log_exception, log_page_progress

from crawler_lib import (
    ConfigManager,
    CompanyCrawler,
    DoubleFetchCrawler
)


class UnifiedCrawler:
    """
    ç»Ÿä¸€çˆ¬è™«ç±»
    
    æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©å•æ¬¡è¯·æ±‚æˆ–äºŒæ¬¡è¯·æ±‚æ¨¡å¼
    """
    
    def __init__(self, exhibition_code: str, max_workers: int = 4, start_page: int = 1):
        self.exhibition_code = exhibition_code
        self.max_workers = max_workers
        self.start_page = start_page
        
        # åŠ è½½é…ç½®
        config_manager = ConfigManager()
        self.config = config_manager.get_config(exhibition_code)
        
        if self.config is None:
            error_msg = f"æœªæ‰¾åˆ°å±•ä¼š '{exhibition_code}' çš„é…ç½®"
            log_error(f"é…ç½®é”™è¯¯: {error_msg}")
            raise ValueError(error_msg)
        
        # æ ¹æ®è¯·æ±‚æ¨¡å¼é€‰æ‹©çˆ¬è™«
        self.request_mode = self.config.request_mode
        
        if self.request_mode == "double":
            # äºŒæ¬¡è¯·æ±‚æ¨¡å¼
            self.crawler = DoubleFetchCrawler(exhibition_code, max_workers, start_page)
        else:
            # å•æ¬¡è¯·æ±‚æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
            self.crawler = CompanyCrawler(exhibition_code, max_workers, start_page)
    
    def crawl(self) -> bool:
        """æ‰§è¡Œçˆ¬å–"""
        try:
            result = self.crawler.crawl()
            return result
            
        except Exception as e:
            log_error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {self.exhibition_code}", exception=e)
            return False

def main():
    """ä¸»å‡½æ•°"""
    program_start_time = datetime.now()
    
    try:
        # è®°å½•ç¨‹åºå¯åŠ¨
        log_info("ğŸš€ å¯åŠ¨ ç»Ÿä¸€çˆ¬è™«ç¨‹åº v3.2")
        
        if len(sys.argv) < 2:
            print("ç”¨æ³•: python run_crawler.py <exhibition_code> [options]")
            print("\né€‰é¡¹:")
            print("  --workers N      å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 2ï¼Œæ¨è1-2é¿å…è§¦å‘é˜²çˆ¬ï¼‰")
            print("  --start-page N   èµ·å§‹é¡µç ï¼ˆé»˜è®¤: 1ï¼‰")
            print("\næ”¯æŒä¸¤ç§æ¨¡å¼:")
            print("  1. å•æ¬¡è¯·æ±‚æ¨¡å¼: ç›´æ¥ä»APIè·å–å®Œæ•´æ•°æ®")
            print("  2. äºŒæ¬¡è¯·æ±‚æ¨¡å¼: å…ˆè·å–åˆ—è¡¨ï¼Œå†è·å–è¯¦æƒ…")
            print("\næ¨¡å¼ç”±config.xlsxä¸­çš„request_modeå­—æ®µå†³å®š")
            print("  - request_mode = 'single' (é»˜è®¤)")
            print("  - request_mode = 'double'")
            print("\nç¤ºä¾‹:")
            print("  python run_crawler.py æ— äººæœºå±•")
            print("  python run_crawler.py æ— äººæœºå±• --workers 8")
            print("  python run_crawler.py æ— äººæœºå±• --start-page 50")
            print("  python run_crawler.py æ— äººæœºå±• --workers 8 --start-page 50")
            sys.exit(1)
        
        # ç¦ç”¨SSLè­¦å‘Š
        urllib3.disable_warnings()
        
        # è§£æå‚æ•°
        exhibition_code = sys.argv[1]
        max_workers = 2  # é»˜è®¤æ”¹ä¸º2ï¼Œé¿å…è§¦å‘é˜²çˆ¬é™åˆ¶
        start_page = 1
        
        if "--workers" in sys.argv:
            idx = sys.argv.index("--workers")
            if idx + 1 < len(sys.argv):
                max_workers = int(sys.argv[idx + 1])
        
        if "--start-page" in sys.argv:
            idx = sys.argv.index("--start-page")
            if idx + 1 < len(sys.argv):
                start_page = int(sys.argv[idx + 1])
        
        try:
            # æµ‹è¯•è¾“å‡ºï¼šç¡®ä¿UIä¸­èƒ½çœ‹åˆ°è¾“å‡º
            print("ğŸ”„ å¼€å§‹åˆ›å»ºçˆ¬è™«å®ä¾‹...", flush=True)
            crawler = UnifiedCrawler(exhibition_code, max_workers=max_workers, start_page=start_page)
            print("ğŸ”„ çˆ¬è™«å®ä¾‹åˆ›å»ºå®Œæˆï¼Œå¼€å§‹çˆ¬å–...", flush=True)
            success = crawler.crawl()
            print("ğŸ”„ çˆ¬å–å®Œæˆ...", flush=True)
            
            if success:
                print("\nâœ“ çˆ¬å–æˆåŠŸï¼", flush=True)
            else:
                print("\nâœ— çˆ¬å–å¤±è´¥", flush=True)
                sys.exit(1)
                
        except ValueError as e:
            print(f"\né”™è¯¯: {e}", flush=True)
            sys.exit(1)
        except Exception as e:
            log_exception(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯")
            print(f"\nçˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", flush=True)
            sys.exit(1)
            
    except Exception as e:
        log_exception("ç¨‹åºå¯åŠ¨å¤±è´¥")
        print(f"\nç¨‹åºå¯åŠ¨å¤±è´¥: {e}", flush=True)
        sys.exit(1)
        
    finally:
        # è®°å½•ç¨‹åºå…³é—­
        runtime = datetime.now() - program_start_time
        log_info(f"ğŸ‘‹ å…³é—­ ç»Ÿä¸€çˆ¬è™«ç¨‹åº (è¿è¡Œæ—¶é—´: {runtime})")

if __name__ == "__main__":
    main()
