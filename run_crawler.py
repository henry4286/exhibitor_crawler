"""
ç»Ÿä¸€çˆ¬è™«ç¨‹åº - æ”¯æŒå•æ¬¡è¯·æ±‚å’ŒäºŒæ¬¡è¯·æ±‚ä¸¤ç§æ¨¡å¼

ä½¿ç”¨æ–¹æ³•:
    python unified_crawler.py <exhibition_code> [options]
    
ç¤ºä¾‹:
    # å•æ¬¡è¯·æ±‚æ¨¡å¼ï¼ˆè‡ªåŠ¨æ ¹æ®é…ç½®åˆ¤æ–­ï¼‰
    python unified_crawler.py æ— äººæœºå±•
    
    # æŒ‡å®šçº¿ç¨‹æ•°
    python unified_crawler.py æ— äººæœºå±• --workers 8
    
    # äºŒæ¬¡è¯·æ±‚æ¨¡å¼ä¼šè‡ªåŠ¨è¯†åˆ«ï¼ˆæ ¹æ®config.xlsxä¸­çš„request_modeå­—æ®µï¼‰
    python unified_crawler.py å†œäº§å“
"""

import sys
import time
import urllib3
from typing import List, Dict, Any

# è®¾ç½®UTF-8ç¼–ç 
import locale
import io

# é‡è®¾æ ‡å‡†è¾“å‡ºä¸ºUTF-8ç¼–ç 
if sys.platform == 'win32':
    # Windowsç³»ç»Ÿç‰¹æ®Šå¤„ç†
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from crawler_lib import (
    ConfigManager,
    CompanyCrawler,
    DoubleFetchCrawler,
    write_status_file
)


class UnifiedCrawler:
    """
    ç»Ÿä¸€çˆ¬è™«ç±»
    
    æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©å•æ¬¡è¯·æ±‚æˆ–äºŒæ¬¡è¯·æ±‚æ¨¡å¼
    """
    
    def __init__(self, exhibition_code: str, max_workers: int = 4, start_page: int = 1):
        self.exhibition_code = exhibition_code
        self.max_workers = max_workers
        self.start_page = start_page
        
        # åŠ è½½é…ç½®
        config_manager = ConfigManager()
        self.config = config_manager.get_config(exhibition_code)
        
        if self.config is None:
            raise ValueError(f"æœªæ‰¾åˆ°å±•ä¼š '{exhibition_code}' çš„é…ç½®")
        
        # æ ¹æ®è¯·æ±‚æ¨¡å¼é€‰æ‹©çˆ¬è™«
        self.request_mode = self.config.request_mode
        
        print(f"ğŸ“‹ æ£€æµ‹åˆ°è¯·æ±‚æ¨¡å¼: {self.request_mode}", flush=True)
        
        if self.request_mode == "double":
            # äºŒæ¬¡è¯·æ±‚æ¨¡å¼
            print("ğŸ”„ ä½¿ç”¨äºŒæ¬¡è¯·æ±‚æ¨¡å¼ï¼ˆå…ˆè·å–åˆ—è¡¨ï¼Œå†è·å–è¯¦æƒ…ï¼‰", flush=True)
            self.crawler = DoubleFetchCrawler(exhibition_code, max_workers, start_page)
        else:
            # å•æ¬¡è¯·æ±‚æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
            print("âœ¨ ä½¿ç”¨å•æ¬¡è¯·æ±‚æ¨¡å¼ï¼ˆç›´æ¥è·å–å®Œæ•´æ•°æ®ï¼‰", flush=True)
            self.crawler = CompanyCrawler(exhibition_code, max_workers, start_page)
    
    def crawl(self) -> bool:
        """æ‰§è¡Œçˆ¬å–"""
        return self.crawler.crawl()



def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python unified_crawler.py <exhibition_code> [options]")
        print("\né€‰é¡¹:")
        print("  --workers N      å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤: 2ï¼Œæ¨è1-2é¿å…è§¦å‘é˜²çˆ¬ï¼‰")
        print("  --start-page N   èµ·å§‹é¡µç ï¼ˆé»˜è®¤: 1ï¼‰")
        print("\næ”¯æŒä¸¤ç§æ¨¡å¼:")
        print("  1. å•æ¬¡è¯·æ±‚æ¨¡å¼: ç›´æ¥ä»APIè·å–å®Œæ•´æ•°æ®")
        print("  2. äºŒæ¬¡è¯·æ±‚æ¨¡å¼: å…ˆè·å–åˆ—è¡¨ï¼Œå†è·å–è¯¦æƒ…")
        print("\næ¨¡å¼ç”±config.xlsxä¸­çš„request_modeå­—æ®µå†³å®š")
        print("  - request_mode = 'single' (é»˜è®¤)")
        print("  - request_mode = 'double'")
        print("\nç¤ºä¾‹:")
        print("  python unified_crawler.py æ— äººæœºå±•")
        print("  python unified_crawler.py æ— äººæœºå±• --workers 8")
        print("  python unified_crawler.py æ— äººæœºå±• --start-page 50")
        print("  python unified_crawler.py æ— äººæœºå±• --workers 8 --start-page 50")
        sys.exit(1)
    
    # ç¦ç”¨SSLè­¦å‘Š
    urllib3.disable_warnings()
    
    # æ¸…ç©ºçŠ¶æ€æ–‡ä»¶
    write_status_file("errorlog.txt", "")
    write_status_file("python_excute_status.txt", "")
    
    # è§£æå‚æ•°
    exhibition_code = sys.argv[1]
    max_workers = 2  # é»˜è®¤æ”¹ä¸º2ï¼Œé¿å…è§¦å‘é˜²çˆ¬é™åˆ¶
    start_page = 1
    
    if "--workers" in sys.argv:
        idx = sys.argv.index("--workers")
        if idx + 1 < len(sys.argv):
            max_workers = int(sys.argv[idx + 1])
    
    if "--start-page" in sys.argv:
        idx = sys.argv.index("--start-page")
        if idx + 1 < len(sys.argv):
            start_page = int(sys.argv[idx + 1])
    
    print(f"\n{'='*60}")
    print(f"ç»Ÿä¸€çˆ¬è™«ç¨‹åº v3.1")
    print(f"{'='*60}")
    print(f"å±•ä¼šä»£ç : {exhibition_code}")
    print(f"çº¿ç¨‹æ•°: {max_workers}")
    print(f"èµ·å§‹é¡µ: {start_page}")
    print(f"{'='*60}\n", flush=True)
    
    try:
        crawler = UnifiedCrawler(exhibition_code, max_workers=max_workers, start_page=start_page)
        success = crawler.crawl()
        
        if success:
            print("\nâœ“ çˆ¬å–æˆåŠŸï¼", flush=True)
        else:
            print("\nâœ— çˆ¬å–å¤±è´¥", flush=True)
            sys.exit(1)
            
    except ValueError as e:
        print(f"\né”™è¯¯: {e}", flush=True)
        sys.exit(1)
    except Exception as e:
        print(f"\nçˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", flush=True)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
