# 爬虫防爬优化说明 v2.0

## 核心策略

**速度优先 + 无限重试**

- 正常请求：**无任何延迟**，全速爬取
- 遇到限流：**自动重试**，指数退避延迟，直到成功

## 重试机制

### 延迟策略（指数退避）

| 重试次数 | 延迟时间 | 说明 |
|---------|---------|------|
| 第1次 | ~3秒 | 3¹ + 随机抖动 |
| 第2次 | ~9秒 | 3² + 随机抖动 |
| 第3次 | ~27秒 | 3³ + 随机抖动 |
| 第4次 | ~81秒（约1.5分钟） | 3⁴ + 随机抖动 |
| 第5次 | ~243秒（约4分钟） | 3⁵ + 随机抖动 |
| 第6次+ | **600秒（10分钟封顶）** | 最大延迟 |

### 计算公式

```python
wait_time = min(3^attempt + random(0, 10), 600)
```

### 限流检测

检测以下关键词触发重试：
- 中文：`频繁`、`限流`、`访问受限`、`请稍后`、`请求过快`
- 英文：`rate limit`、`too many`、`forbidden`、`throttle`

检测以下响应特征：
- `success: false`
- `code` 不是 0/200
- `status` 是 0/false/error
- `error` 字段非空

## 架构设计

```
┌─────────────────────────────────────────────────────────────┐
│                     http_client.py                          │
│  ┌─────────────────────────────────────────────────────────┐│
│  │  send_request_with_retry()   ← 统一重试逻辑             ││
│  │    - 无限重试，直到成功                                  ││
│  │    - 指数退避延迟（3秒→9秒→27秒→...→10分钟）            ││
│  │    - 业务层+HTTP层限流检测                               ││
│  └─────────────────────────────────────────────────────────┘│
└─────────────────────────────────────────────────────────────┘
              ↑                           ↑
              │                           │
    ┌─────────┴────────┐        ┌────────┴─────────┐
    │   crawler.py     │        │ detail_fetcher.py│
    │  (列表页请求)     │        │  (详情页请求)    │
    └──────────────────┘        └──────────────────┘
```

## 使用示例

```bash
# 标准运行（推荐）
python run_crawler.py 展会名称

# 指定线程数
python run_crawler.py 展会名称 --workers 4

# 从指定页开始
python run_crawler.py 展会名称 --start-page 10
```

## 日志输出示例

### 正常情况

```
📄 第1页 - 步骤1: 获取公司列表
✅ 获取到 20 个公司
📞 第1页 - 步骤2: 抓取联系人
✅ 批量获取完成，成功: 20, 失败: 0
```

### 触发限流重试

```
⚠️ 联系人[某公司] 限流，第1次重试，等待3秒... (业务层限流)
⚠️ 联系人[某公司] 限流，第2次重试，等待9秒... (业务层限流)
⚠️ 联系人[某公司] 限流，第3次重试，等待27秒... (业务层限流)
✅ 联系人[某公司] 第4次重试成功
```

## 优化效果

| 项目 | v1.0 | v2.0 |
|------|------|------|
| 正常请求延迟 | 0.1-0.3秒 | **无延迟** |
| 最大重试次数 | 3次 | **无限制** |
| 重试延迟 | 2,4,6秒(线性) | **3,9,27...秒(指数)** |
| 最大延迟 | 6秒 | **600秒(10分钟)** |
| 数据保证 | 可能丢失 | **必定成功** |
| 列表页重试 | 无 | **有** |
| 详情页重试 | 有 | **有** |

## 技术细节

### 统一重试方法

所有HTTP请求统一调用 `HttpClient.send_request_with_retry()`：

```python
response_data = HttpClient.send_request_with_retry(
    url=url,
    method='GET/POST',
    headers=headers,
    params=params,
    data=data,
    context="上下文描述"  # 用于日志输出
)
# 必定返回成功的响应数据
```

### 关键代码位置

- 重试机制：`crawler_lib/http_client.py` → `send_request_with_retry()`
- 延迟计算：`crawler_lib/http_client.py` → `calculate_retry_delay()`
- 限流检测：`crawler_lib/http_client.py` → `is_rate_limited_response()`

## 注意事项

1. **无限重试特性**：如果目标网站完全封禁，程序会持续重试（最长每10分钟一次）
2. **中断恢复**：已保存的数据不会丢失，可使用 `--start-page` 从断点继续
3. **并发控制**：默认2线程，可根据目标网站承受能力调整
