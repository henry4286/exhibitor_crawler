import requests
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
from urllib.parse import urlencode  # ä¿®å¤ requests.utils.urlencode çš„é—®é¢˜
import warnings
import json # ç”¨äºç¾åŒ–è¾“å‡ºï¼Œæ–¹ä¾¿è°ƒè¯•

# å¿½ç•¥ SSL è¯ä¹¦éªŒè¯è­¦å‘Š
warnings.filterwarnings("ignore")

# --- å…¨å±€é…ç½® ---
BASE_URL = "https://qpz.sinomachint.com/CommonJson/GetCZJson"
DETAIL_URL ="https://qpz.sinomachint.com/CommonJson/GetCZXQJson"
TOTAL_PAGES = 342  # æ€»é¡µæ•°
MAX_WORKERS = 4   # å¹¶å‘çº¿ç¨‹æ•°
EXCEL_FILE = "å…¬å¸åŠè”ç³»äººä¿¡æ¯.xlsx"
HEADERS = {
    "Content-Type": "application/x-www-form-urlencoded"
}

# ç”¨äºä¿å­˜å…¬å¸IDåˆ—è¡¨ï¼Œé¿å…é‡å¤æŠ“å–
company_ids_set = set()


# ==============================================================================
# 0. éªŒè¯ä¸ä¿å­˜åŠŸèƒ½
# ==============================================================================

def validate_response(response_json: dict, request_type: str) -> bool:
    """
    éªŒè¯å“åº”ä½“ä¸­çš„ 'success' é”®å€¼æ˜¯å¦ä¸º Trueã€‚
    """
    if response_json.get("success") is True:
        return True
    else:
        # å¦‚æœ success ä¸ä¸º Trueï¼Œæ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
        print(f"--- âš ï¸ {request_type} å“åº”ä½“éªŒè¯å¤±è´¥ ---")
        try:
            print(f"å“åº”å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰: {str(response_json)[:200]}")
        except Exception:
            pass
        return False

def append_to_excel(data_list: list, filename: str):
    """
    å°†æ–°æ•°æ®è¿½åŠ ä¿å­˜åˆ° Excel æ–‡ä»¶ä¸­ã€‚
    """
    if not data_list:
        return

    new_df = pd.DataFrame(data_list)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if os.path.exists(filename):
        try:
            # å°è¯•è¿½åŠ æ¨¡å¼
            with pd.ExcelWriter(filename, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:
                 # è·å–ç°æœ‰æ•°æ®çš„è¡Œæ•°ï¼Œä»ä¸‹ä¸€è¡Œå¼€å§‹å†™å…¥ï¼Œä¸å¸¦è¡¨å¤´
                 start_row = writer.sheets['å…¬å¸ä¿¡æ¯'].max_row
                 new_df.to_excel(writer, sheet_name='å…¬å¸ä¿¡æ¯', startrow=start_row, header=False, index=False)
            
        except Exception:
            # å¦‚æœè¿½åŠ å¤±è´¥ï¼ˆå¦‚æ–‡ä»¶è¢«å ç”¨æˆ–é¦–æ¬¡å†™å…¥ï¼‰ï¼Œåˆ™ä½¿ç”¨è¦†ç›–æ¨¡å¼å†™å…¥
            print(f"è¿½åŠ å†™å…¥ Excel å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨è¦†ç›–æ¨¡å¼å†™å…¥ã€‚")
            new_df.to_excel(filename, sheet_name='å…¬å¸ä¿¡æ¯', index=False)
            
    else:
        # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥åˆ›å»ºå¹¶å†™å…¥ï¼ŒåŒ…å«è¡¨å¤´
        new_df.to_excel(filename, sheet_name='å…¬å¸ä¿¡æ¯', index=False)


# ==============================================================================
# I. å…¬å¸åˆ—è¡¨ä¿¡æ¯è·å–ä¸è§£æ
# ==============================================================================

def get_company_list_response(page_num: int) -> dict:
    """
    å‘é€è¯·æ±‚è·å–å…¬å¸åˆ—è¡¨ä¿¡æ¯ï¼Œå¹¶éªŒè¯å“åº”ä½“ã€‚
    """
    url = BASE_URL
    data = {
        "TName": "MTZH",
        "CurrentPage": page_num
    }
    
    # ä½¿ç”¨ urlencode è½¬æ¢æ•°æ®
    request_data = urlencode(data)
    
    response = requests.post(url, data=request_data, headers=HEADERS, verify=False) # ç¦ç”¨ SSL éªŒè¯
    response.raise_for_status() # æ£€æŸ¥ HTTP çŠ¶æ€ç 
    
    response_json = response.json()
    
    # éªŒè¯ 'success' å­—æ®µ
    if not validate_response(response_json, f"ç¬¬ {page_num} é¡µåˆ—è¡¨è¯·æ±‚"):
        raise ValueError(f"å“åº”ä½“éªŒè¯å¤±è´¥ï¼ŒPage {page_num}ã€‚")
        
    return response_json


def parse_company_ids(response_json: dict) -> list:
    """
    ä»å…¬å¸åˆ—è¡¨å“åº”ä½“ä¸­è§£æå‡ºæ‰€æœ‰å…¬å¸ IDã€‚
    """
    ids = []
    try:
        table_keys = response_json.get("jsonData", {}).get("TableKeys", [])
        if table_keys:
            ids = [int(key) for key in table_keys]
    except Exception as e:
        print(f"è§£æå…¬å¸IDæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return []
        
    return ids


# ==============================================================================
# II. å•ä¸ªå…¬å¸è¯¦æƒ…ä¿¡æ¯è·å–ä¸è§£æ
# ==============================================================================

def get_company_details_response(company_id: int) -> dict:
    """
    å‘é€è¯·æ±‚è·å–å•ä¸ªå…¬å¸åŠè”ç³»äººä¿¡æ¯ï¼Œå¹¶éªŒè¯å“åº”ä½“ã€‚
    """
    url = DETAIL_URL
    data = {
        "ID": str(company_id),
        "tablename": "MTZH",
        "zk": "CZSZC",
        "col": "ID,DWBH,MTZHGSMC,gsbzvalue,MTZHGSDZ,MTZHWZ,MTZHEMAIL,gsjjvalue,MTZHGSJJ,MTZHCPFL,MTZHCPJJ1,cp1value,MTZHCPJJ2,cp2value,MTZHCPJJ3,cp3value,MTZHCPJJ4,cp4value,MTZHCPJJ5,cp5value,MTZHGJZ,MTZHGSJJEN,MTZHCPJJEN1,MTZHCPJJEN2,gsjjvalue,MTZHCPJJEN3,MTZHCPJJEN4,MTZHCPJJEN5"
    }
    
    # ä½¿ç”¨ urlencode è½¬æ¢æ•°æ®
    request_data = urlencode(data)
    
    response = requests.post(url, data=request_data, headers=HEADERS, verify=False) # ç¦ç”¨ SSL éªŒè¯
    response.raise_for_status() # æ£€æŸ¥ HTTP çŠ¶æ€ç 
    
    response_json = response.json()
    
    # éªŒè¯ 'success' å­—æ®µ
    if not validate_response(response_json, f"å…¬å¸ID {company_id} è¯¦æƒ…è¯·æ±‚"):
        raise ValueError(f"å“åº”ä½“éªŒè¯å¤±è´¥ï¼ŒID {company_id}ã€‚")

    return response_json


def parse_company_details(response_json: dict) -> dict:
    """
    ä»å…¬å¸è¯¦æƒ…å“åº”ä½“ä¸­è§£æå‡ºå…¬å¸åç§°ã€å±•ä½å·ã€ç”µè¯å’Œé‚®ç®±ã€‚
    """
    company_info = {
        "å…¬å¸ID": "N/A",
        "å…¬å¸åç§°": "N/A",
        "å±•ä½å·": "N/A",
        "ç”µè¯": "N/A",
        "é‚®ç®±": "N/A"
    }
    
    try:
        # è§£æ jsonData.RowValues
        row_values = response_json.get("jsonData", {}).get("RowValues", [])
        if len(row_values) >= 7:
            company_info["å…¬å¸ID"] = row_values[0] # ID
            company_info["å…¬å¸åç§°"] = row_values[2] # MTZHGSMC
            company_info["é‚®ç®±"] = row_values[6] # MTZHEMAIL
        
        # è§£æ row åˆ—è¡¨è·å–å…¶ä»–ä¿¡æ¯
        dh1, dh2, dh3 = None, None, None
        
        for item in response_json.get("row", []):
            name = item.get("Name")
            value = item.get("Value", "").strip()
            
            if name == "ZWH":
                company_info["å±•ä½å·"] = value
            elif name == "DH1": 
                dh1 = value
            elif name == "DH2": 
                dh2 = value
            elif name == "DH3": 
                dh3 = value

        # ç»„åˆç”µè¯å·ç 
        # å°è¯•å°†ç”µè¯ç»„åˆæˆ +86 0563 6987688 æ ¼å¼
        if dh1 and dh2 and dh3 and dh1 != " " and dh2 != " " and dh3 != " ":
            # å¦‚æœåœ°åŒºç ï¼ˆDH2ï¼‰ä¸æ˜¯0å¼€å¤´ï¼Œå°è¯•åœ¨å‰é¢åŠ 0
            formatted_dh2 = f"0{dh2}" if len(dh2) > 0 and dh2[0] != '0' else dh2
            company_info["ç”µè¯"] = f"+{dh1} {formatted_dh2} {dh3}"
        
    except Exception as e:
        print(f"è§£æå…¬å¸è¯¦æƒ…æ—¶å‘ç”Ÿé”™è¯¯: {e}ï¼ŒåŸå§‹æ•°æ®: {response_json}")
        return company_info

    return company_info


# ==============================================================================
# III. å¹¶å‘æ‰§è¡Œä¸ä¸»ç¨‹åº
# ==============================================================================

def fetch_company_details(company_id: int):
    """
    è·å–å•ä¸ªå…¬å¸è¯¦æƒ…å¹¶è¿”å›è§£æåçš„æ•°æ®ã€‚
    """
    try:
        detail_json = get_company_details_response(company_id)
        return parse_company_details(detail_json)
    except (requests.exceptions.RequestException, ValueError) as e:
        # æ•è·ç½‘ç»œé”™è¯¯å’ŒéªŒè¯å¤±è´¥
        print(f"å…¬å¸ID {company_id} è¯¦æƒ…è¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        print(f"å…¬å¸ID {company_id} è¯¦æƒ…å¤„ç†å¤±è´¥: {e}")
    return None

def process_page(page_num: int):
    """
    å¤„ç†å•ä¸ªé¡µé¢çš„é€»è¾‘ï¼šè·å–å…¬å¸IDï¼Œç„¶åå¹¶å‘è·å–å…¬å¸è¯¦æƒ…ï¼Œæœ€åä¿å­˜ã€‚
    """
    print(f"--- ğŸš€ æ­£åœ¨å¤„ç†ç¬¬ {page_num}/{TOTAL_PAGES} é¡µ ---")
    
    try:
        # 1. è·å–åˆ—è¡¨å“åº”ä½“å¹¶éªŒè¯
        list_json = get_company_list_response(page_num)
        
        # 2. è§£æå…¬å¸ ID åˆ—è¡¨
        new_company_ids = parse_company_ids(list_json)
        
        if not new_company_ids:
            print(f"ç¬¬ {page_num} é¡µæœªè§£æåˆ°å…¬å¸ ID æˆ–åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
            return
            
        ids_to_fetch = [id for id in new_company_ids if id not in company_ids_set]
        
        # 3. å¹¶å‘è·å–å…¬å¸è¯¦æƒ…
        page_results = []
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_id = {executor.submit(fetch_company_details, id): id for id in ids_to_fetch}
            
            for future in as_completed(future_to_id):
                company_id = future_to_id[future]
                result = future.result()
                if result:
                    page_results.append(result)
                    company_ids_set.add(company_id) # æ ‡è®°ä¸ºå·²å®Œæˆ
        
        # 4. å®æ—¶ä¿å­˜æ•°æ®åˆ° Excel
        if page_results:
            append_to_excel(page_results, EXCEL_FILE)
            print(f"--- âœ… ç¬¬ {page_num} é¡µå¤„ç†å®Œæˆï¼ŒæˆåŠŸæŠ“å– {len(page_results)} æ¡è®°å½•å¹¶ä¿å­˜åˆ° Excelã€‚ ---")
        else:
            print(f"--- âš ï¸ ç¬¬ {page_num} é¡µå¤„ç†å®Œæˆï¼Œä½†æœªæˆåŠŸæŠ“å–åˆ°æ–°è®°å½•ã€‚ ---")
            
    except (requests.exceptions.RequestException, ValueError) as e:
        print(f"--- âŒ ç¬¬ {page_num} é¡µåˆ—è¡¨è¯·æ±‚å¤±è´¥æˆ–éªŒè¯å¤±è´¥: {e} ---")
    except Exception as e:
        print(f"--- âŒ ç¬¬ {page_num} é¡µå¤„ç†æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e} ---")


def main_scraper():
    """
    ä¸»ç¨‹åºå…¥å£ï¼Œä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ‰€æœ‰é¡µé¢ã€‚
    """
    print(f"--- ğŸŒ å¯åŠ¨æ•°æ®æŠ“å–ç¨‹åº ---")
    print(f"ç›®æ ‡æ€»é¡µæ•°: {TOTAL_PAGES}")
    print(f"å¹¶å‘çº¿ç¨‹æ•°: {MAX_WORKERS}")
    print(f"ä¿å­˜æ–‡ä»¶è·¯å¾„: {EXCEL_FILE}")
    
    # ç¡®å®šè¦å¤„ç†çš„é¡µç èŒƒå›´
    page_numbers = range(1, TOTAL_PAGES + 1)

    # ä½¿ç”¨å¤šçº¿ç¨‹æ¥å¹¶å‘å¤„ç†æ¯ä¸€é¡µ
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_page, page) for page in page_numbers]
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in as_completed(futures):
            # ç®€å•è¿­ä»£ä»¥ç¡®ä¿æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæ¯•
            pass

    print("\n--- ğŸ‰ æ‰€æœ‰é¡µé¢æ•°æ®æŠ“å–å’Œä¿å­˜å·¥ä½œå®Œæˆï¼ ---")


# ==============================================================================
# V. æ‰§è¡Œä¸»ç¨‹åº
# ==============================================================================
if __name__ == "__main__":

    main_scraper()